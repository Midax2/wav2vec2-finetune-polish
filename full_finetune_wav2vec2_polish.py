import json

import tensorflow as tf
import torch
import torch.onnx
from datasets import Dataset, Audio
from transformers import (
    Wav2Vec2CTCTokenizer,
    Wav2Vec2Processor,
    Wav2Vec2ForCTC,
    TrainingArguments,
    Trainer,
    TFWav2Vec2ForCTC,
)
from transformers.utils import logging

# Use this to see Hugging Face logs if needed
logging.set_verbosity_info()

# --------- 1. Prepare dataset ---------
# Define your audio data and corresponding text labels
data = [
    {"audio": "data/audio/zako.wav", "text": "projekt na zako"},
    {"audio": "data/audio/dzien_dobry.wav", "text": "dzień dobry"}
]

# Create a Hugging Face Dataset from your data
dataset = Dataset.from_list(data)
# Cast the audio column to the Audio feature with the desired sampling rate (16kHz for Wav2Vec2)
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))

# --------- 2. Create custom tokenizer ---------
# Define the vocabulary for the tokenizer, including Polish specific characters
vocab = list("abcdefghijklmnopqrstuvwxyząćęłńóśźż '")
# Create a dictionary mapping characters to their integer IDs
vocab_dict = {v: k for k, v in enumerate(vocab)}
# Use '|' as the word delimiter token, replacing the space character for better distinction
vocab_dict["|"] = vocab_dict[" "]
del vocab_dict[" "]  # Remove the original space entry

# Save the custom vocabulary to a JSON file. This file will be used by the tokenizer.
# The 'ensure_ascii=False' argument is crucial for correctly saving Polish characters.
with open("vocab.json", "w", encoding="utf-8") as f:
    json.dump(vocab_dict, f, ensure_ascii=False)

# Initialize the Wav2Vec2CTCTokenizer with the custom vocabulary.
# '[UNK]' is for unknown tokens, '[PAD]' for padding, and '|' as the word delimiter.
tokenizer = Wav2Vec2CTCTokenizer(
    "vocab.json",
    unk_token="[UNK]",
    pad_token="[PAD]",
    word_delimiter_token="|"
)
# Save the tokenizer to a directory for later use and to be loaded by the processor
tokenizer.save_pretrained("tokenizer/")

# --------- 3. Load model and processor ---------
# Load the Wav2Vec2Processor. This integrates the feature extractor and the tokenizer.
# We're using a pre-trained 'facebook/wav2vec2-base' and our custom tokenizer.
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base", tokenizer=tokenizer)
# Load the Wav2Vec2ForCTC model. 'vocab_size' is set to the size of our custom tokenizer's vocabulary.
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base", vocab_size=len(tokenizer))

# --------- 4. Preprocess data ---------
def prepare_batch(batch):
    """
    Preprocesses a single batch (example) of audio and text data.
    It converts raw audio arrays into model-ready input features and text transcripts into tokenized labels.
    """
    audio = batch["audio"]["array"]
    # Process audio: convert to input features (e.g., log-mel spectrograms).
    # 'return_tensors="pt"' ensures PyTorch tensors are returned.
    inputs = processor(audio, sampling_rate=16000, return_tensors="pt", padding=True)

    # Process labels (text): convert text to token IDs.
    # The 'as_target_processor()' context manager ensures that the tokenizer's
    # special tokens (like pad_token) are handled correctly for labels.
    with processor.as_target_processor():
        labels = processor(batch["text"], return_tensors="pt", padding=True).input_ids

    # Assign processed inputs and labels to the batch dictionary.
    # '.squeeze(0)' removes the batch dimension, as 'prepare_batch' is applied to single examples.
    batch["input_values"] = inputs["input_values"].squeeze(0)
    # Handle attention mask if it's generated by the processor
    if "attention_mask" in inputs and inputs["attention_mask"] is not None:
        batch["attention_mask"] = inputs["attention_mask"].squeeze(0)
    else:
        batch["attention_mask"] = None
    batch["labels"] = labels.squeeze(0)  # Labels are now a 1D tensor

    return batch

# Apply the preprocessing function to the entire dataset.
# 'remove_columns' helps save memory by dropping original audio and text data after processing.
train_dataset = dataset.map(prepare_batch, remove_columns=["audio", "text"])

# --------- 5. Define data collator ---------
def data_collator(batch):
    """
    Custom data collator to pad sequences to the longest length within each batch.
    This is essential for batching sequences of varying lengths in PyTorch.
    It pads input_values, attention_mask, and labels to the maximum length in the current batch.
    """
    # Convert 'input_values' from potentially lists to PyTorch tensors and then pad them.
    input_values = [
        torch.tensor(item["input_values"]) if isinstance(item["input_values"], list) else item["input_values"]
        for item in batch
    ]
    input_values = torch.nn.utils.rnn.pad_sequence(
        input_values,
        batch_first=True,  # Pad along the batch dimension
        padding_value=processor.tokenizer.pad_token_id, # Use the tokenizer's pad token ID for padding
    )

    attention_mask = None
    if batch[0]["attention_mask"] is not None:
        # Convert 'attention_mask' from potentially lists to PyTorch tensors and then pad them.
        attention_mask = [
            torch.tensor(item["attention_mask"]) if isinstance(item["attention_mask"], list) else item["attention_mask"]
            for item in batch
        ]
        attention_mask = torch.nn.utils.rnn.pad_sequence(
            attention_mask,
            batch_first=True,
            padding_value=0, # Use 0 for padding in the attention mask
        )

    # Convert 'labels' from potentially lists to PyTorch tensors and then pad them.
    labels = [
        torch.tensor(item["labels"]) if isinstance(item["labels"], list) else item["labels"]
        for item in batch
    ]
    labels = torch.nn.utils.rnn.pad_sequence(
        labels,
        batch_first=True,
        padding_value=processor.tokenizer.pad_token_id, # Use the tokenizer's pad token ID for labels
    )

    return {
        "input_values": input_values,
        "attention_mask": attention_mask,
        "labels": labels,
    }

# --------- 6. Training ---------
# Define training arguments, specifying output directory, batch size, epochs, etc.
training_args = TrainingArguments(
    output_dir="./wav2vec2-pl",  # Directory to save model checkpoints and logs
    per_device_train_batch_size=2, # Number of samples per training batch per device
    num_train_epochs=5, # Total number of training epochs
    save_steps=10, # Save model checkpoint every 10 steps
    logging_steps=5, # Log training metrics (loss, etc.) every 5 steps
    learning_rate=1e-4, # Initial learning rate for the optimizer
    fp16=torch.cuda.is_available(), # Enable mixed-precision training if a CUDA GPU is available
    report_to=None # Disable reporting to external services like Weights & Biases
)

# Initialize the Hugging Face Trainer with the model, training arguments, dataset, and data collator.
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator,
    tokenizer=processor, # Processor is passed here for tokenization during evaluation (though not explicitly used for this script)
)

# Start the training process
trainer.train()
# Save the fine-tuned model and processor after training is complete
trainer.save_model("wav2vec2-pl")
processor.save_pretrained("wav2vec2-pl")

# Reload the processor and model for conversion to TensorFlow
processor = Wav2Vec2Processor.from_pretrained("wav2vec2-pl")
# Load the fine-tuned PyTorch model into a TensorFlow model
tf_model = TFWav2Vec2ForCTC.from_pretrained("wav2vec2-pl", from_pt=True)

# Define a TensorFlow serving signature for the model
@tf.function(input_signature=[tf.TensorSpec([None, None], tf.float32, name="input_values")])
def model_serving(input_values):
    # Pass the input values through the TensorFlow model
    outputs = tf_model(input_values)
    # Return the logits, which are the raw predictions before softmax
    return {"logits": outputs.logits}

# Save the TensorFlow model in the SavedModel format
saved_model_dir = "tf_model"
tf.saved_model.save(tf_model, saved_model_dir, signatures={"serving_default": model_serving})

print("Model successfully converted and saved as TensorFlow SavedModel (tf_model/)")

# Convert the TensorFlow SavedModel to a TFLite model, applying default optimizations (quantization)
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT] # This enables default optimizations, including quantization
tflite_model = converter.convert()

# Save the TFLite model to a file
with open("final_model_polish_quant.tflite", "wb") as f:
    f.write(tflite_model)

print("Model successfully converted and saved as final_model_polish_quant.tflite")